---
title: "Memory Without the Headache: Rethinking Context in Large Language Models"
description: "Why do LLMs require the entire conversation history with each API call—and are there better ways to handle memory? Herman Poppleberry explores the limits of context windows, the illusion of state, and..."
pubDate: "2025-08-27"
heroImage: "https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/44222638/44222638-1764854255410-85c4a5dd24773.jpg"
tags: ["Llm", "Api", "Prompt"]
prompt: "Episode from My Weird Prompts podcast"
podcastAudioUrl: "https://res.cloudinary.com/drrvnflqy/video/upload/v1764942777/cd7750c32a892e092e190f3b4b6f7f510a/memory-without-the-headache-rethinking-context-in-large-language-models.mp3"
podcastDuration: "7:15"
aiGenerated: true
migratedFromAnchor: true
originalGuid: "0f814101-8ccf-4db8-a16b-9aa4f42abe3b"
---

## About This Episode

Why do LLMs require the entire conversation history with each API call—and are there better ways to handle memory? Herman Poppleberry explores the limits of context windows, the illusion of state, and emerging alternatives like prompt caching, memory tokens, and hierarchical summarization.

## Listen

<audio controls src="https://res.cloudinary.com/drrvnflqy/video/upload/v1764942777/cd7750c32a892e092e190f3b4b6f7f510a/memory-without-the-headache-rethinking-context-in-large-language-models.mp3" style="width: 100%;">
  Your browser does not support the audio element.
</audio>

**Duration:** 7:15

---

*This episode was migrated from the original podcast feed. It features AI-generated dialogue exploring prompts and questions submitted by Daniel Rosehill.*
