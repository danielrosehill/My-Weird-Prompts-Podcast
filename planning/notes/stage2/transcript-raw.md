# Stage 2 Notes - Raw Transcript

**Recorded**: December 2025
**Source**: notes-2.mp3

---

Okay, so I'm going to record just a little bit more context data again, just um the AI podcast experiment, great AI podcasting experiment. Um and I'm doing so because at the end of the day, I'm going to come back from walking for a couple of hours. I had a really, really nice walk and there's one thing that I think I need to do more of, besides, you know, um, getting healthy, spending time with family. This is the stuff I'm trying to do more of in 2026. I love going for walks. And, um, I see so much potential in this idea for an AI podcast. It's actually something that I'm creating for myself, I guess, like a lot of things. Um, because during the course of the day, I come up with questions that are very detailed about specific things. Um, at the moment, I think when I was out, I recorded one about, um, control nets in generative AI. And it's the kind of information that I just find that it's not really fun to read big long responses at like a specific time. But, um, when they get packaged up in like an audio episode, you're very engaged because it's something you really wanted to know exactly. Um, and it kind of immediately pulls you in. Um, and it's a great way to for you'll get long form content made in a way that works. So, it's envisioning it really as an as an as a personal way of actually consuming information and learning uh from AI, getting those responses that are kind of very lengthy um based on lengthy inputs in turn. And the challenge I think is that, so the workflow is as I've we we got it to work um so far in this. Um, tried out local text to speech, that didn't really work that well, or was very hard uh to get that working well. Um, especially for the kind of two participant model that I've kind of based on here, where you have like, um, two characters with each of a agreed to silly voice clones. And that's more fun than one, I find, more enjoyable, or I guess I get a kick out of it because it's just these two kind of stuffed animals in our house that I've kind of brought to life in this podcast and it's it's kind of it's it it's funny for me to listen to them them banter in this ridiculous AI way. Um, and I guess with Google, maybe I was biased towards the idea of a two part podcast based on how NotebookLM have done this, which is uh two, two hosts. Um, so local TTS Cocoro, there's a bunch of models that should work, um, and should be able to work, but I think with AMD, it was just like it was one problem after the other trying to get it to work. And the only reason I was doing that in the first place was really this is something I want to I want to record a few of these per day. And if it's going to cost like 10 bucks per day, it's very quickly going to become like, not viable to do this because they're going to be long episodes, multiple turns for each person. And a lot of time in me in trying to figure out this idea so far has been trying to find, um, a TTS model that isn't painfully bad, because it's so robotic, which is what's cheap, or really expensive and natural. And that's kind of why local TTS seemed like a potential direction. So, as we worked on this project on Friday, where we got to was, um, validated it with from a few directions. Uh, I recorded a couple of voice samples for these uh character voices. And the cloning was was was was fine. I mean, with a fake voice for an imaginary person, you can't really, you can't really do a proper training set. So like, I think probably zero shot is as good as you'll get. And but it worked fine. Like the voices were great. The scripts were good and informative and I've enjoyed listening to them. Um, a lot actually. Some really good ones that I listened to the last few days. I I I went through my queue and now what I'm looking at really is just kind of making the system like viable to run as something I do all the time. Like I when I recorded a voice note now, um, I want to get it all the way through this pipeline basically with minimal interventions on my part. Not because I'm I'm lazy, this isn't for, again, it's like it's it's public because I think some of the episodes are actually very interesting. Uh, but I want to make it easy for me to generate these discussions so that I can just record something and then I know, oh great, that's going to be generated. Doesn't have to be immediate, could be in an hour. Um, there's no real it's never something I kind of it's more like I create a few and kind of savor listening to them. Um, as they're kind of queued up in the background and I know they'll make their way. So that's kind of how I envision it anyway. Um, so where do we get to? Replicate Chatterbox was great. Very impressive model. And I think we did the cost calculations for one of the episodes and it wasn't really that that much. Gemini, um, was what I initially wanted to use for this, their multimodal AI. And the reason Gemini has a lot to offer in the sense that it's it's because it's multimodal it can do a lot of steps including text to speech. But the text to speech uh doesn't support voice cloning, and I really dislike the models that they have in their library, all of them actually. Um, they're very, I don't know, it's a certain kind of very like stuck over optimistic West California accent across genders that I just find kind of very grating for some reason. And I just don't it's like it it ruins the the podcast for me basically, those voices. Um, thinking of my friend from California and saying, I'm sorry. I like your voice, just not these voices. So the replicate cloned script, I think was the best. Uh, and what we got to, where we got to in the last day was that I identified a a kind of what seemed a very silly, it still seems a bit silly to me. Let's say I want to do a voice, I have two voices here in this repository. Corn the sloth and Herman the donkey. These are my two stuffed animals who I uh play around with in the in the podcasting experiment. Corny, they have each got their own kind of unique weird voices, uh which are in this in this repository. And the way it was working with Chatterbox, I realized was what would make sense to me would be you kind of create almost like a Laura in um generative in image AI. You do that once like you would for a proper voice clone and that's it, you have your voice. What I so what was happening when looking at the at the runs was it was doing this zero shot process on every single turn. So, it was reuploading the voice sample of the sloth generating from that. It just seemed very, very inefficient. Um, but it did work. And the script um concatenated. That was that was the essential element, or that's what I want to have is my audio concatenated, because as much as it adds a little bit of complexity and that we need to pull it together uh somehow, concatenated. Um, it makes it less just a purely robotic podcast and there's actually human me sending in the prompt, which is what's really happening, of course. Um, so, when I come back to, so let's say this works fine in the repository, which it does. Um, so what I really need or want is a way to a pipeline, a way that I can record these prompts. What I did before for my previous experiments was I have I use an app called voice notes and it allows you to create web hooks, and I tagged, I created a web hook for the podcast generation queue tag that hits N8N. And then it did this. Now the issue with N8N is that I find that you can never really guess, like it took some iterations here in this environment for us to ready you Claude to figure out this and for me to debug and kind of nudge you in directions, but we got there. And I think the reason is you defined a virtual environment, we worked through the package packages and it's replicable. And I always find in any then, I kind of it's still a great platform, but over time sort of felt like I actually much prefer just using AI agents to work directly in Python because I feel like you're it just opens up um a whole lot more possibilities. I still find I'm lacking a platform, but that's kind of what I'm leaning towards. So really what I would like to do is let's take this repository as it exists here. And the uh the only stage in this chain that I can't that I don't uh want to automate currently is just the publishing. After the audio, after the episode gets generated, actually um publishing it as a podcast, which I put on Spotify again because that means I I can listen to it or I can send it to a friend and say, hey, this was I got this response from AI. I thought it was really cool, here's a link. So I can't do that if it's private, obviously. Um, so what I do currently is I I was running this pipe, recording my prompts, sending it to the pipeline, getting the episode, manually uploading it from this repository to Spotify. Great. I can do that, but it's not a great workflow because I need to be like at my computer while the scripts are running, and then I need to be at my computer to use GitHub. Really, um, so the only way the last step could be automated was through an API that let you publish to a podcast and the only one I found was Transistor FM, which is kind of expensive. But down the road, I think that that could be the way to go probably. I could use a self-hosted podcast uh manager, but I don't really mind them bundling into uh into a um Google Drive folder as the output, where I can or even just like a an object storage on the on the server. So long as I know what's happened to like notification. Um, and the outputs were the podcast episode, the title and the description because that makes it really easy for me to just like pop those fields into the podcast publisher and share the episode, publish it. And of course, I can listen back and just make sure it came through okay. Um, but those are kind of the two ends really for the workflow. So, where I where I would like to be with this is that I'm at I'm in the market like I was just now, and I say, oh, I have a question. And I'm not going to type open up chatT on my phone. Uh, I'm just going to record while I'm handsfree. And I record like a long prompt, 5 minutes and 4 minutes. And that's the I'll say, oh, this is this is a podcast episode. I'd love to hear my two little characters discussing this question and giving me answering it. And then I'll once I have that, I'll listen to that at the gym tonight. That's what I'm trying to get to. Um, and that would kind of ideally end to end, I guess, let's say with Transistor FM, a little notification saying, hey, you the your, let's call it the the lower the control net podcast, it's ready, here's the link. And I'm like great. That's like that that's that's the full end to end uh process. I literally just get an email saying it's been and all the back end work of the TTS and the LLM happens in the background. Uh, even concatenates my prompts because it gets the audio binary. And uh, and that's it, happy days. Um, so if there is anything, the issue with Transistor FM is really that I'm trying to uh it's just that it's another like $20 a month subscription that I'm kind of drowning in SAS subscriptions at the moment. Um, so it's not that I don't think it's a useful service, but I'm not sure they even really support programmatic publishing in the way I'm kind of envisioning it. I'm not sure really who does. Like I don't want to I'd try to I would love to find a publishing platform that kind of like supports what I'm trying to do because I think it's easy to miss like on the outside. I don't know, this might look like I'm just trying to create a really spammy AI generated podcast just to like like a low effort play to like build up a big reputation or something, which is literally the complete opposite of what I'm doing. I'm trying to create informative learning content for myself and for anyone, you know, else who might find it interesting with the clear disclaimer that it's AI generated, so you want to check the info. But generally with Cloud, I found it's 99% there. I would argue probably the accuracy is at least on par with a very educated host. Even humans get stuff wrong, right? So, um, I feel comfortable with that disclaimer having it out there fully in the public domain. Um, and yeah, that's kind of where I'd like to see it. Um, and I think we can sort of discard some of the failed approaches locally and just focus on ways this could be deployed to a whatever environment that makes sense. And I'm drawing a blank here because I know there's stuff like serverless. Um, and I don't really have that experience. What I'm trying to get to with my N8N stuff is I'm already feeling kind of like good learning curve, but I'd be really happy to just have like a a pure pure code automation server or maybe a hybrid model. Um, but I guess we do need something that's persistently accessible for the webhook uh to get in, however that gets in, maybe it's a custom app that we built for capturing these prompts, but it seems overkill to build an app just for that single purpose. But probably be capturing them sometimes from my computer, sometimes from my phone. Running that through the pipeline, the audio processing pipeline and putting it out. So, that's really where I'm kind of asking turning it over to you. Um, what's out there that how can we, how can we get this to that format which will enable me to actually start sending in a few prompts per day, getting a few outputs per day. You know, if it cost like $3, $4 per day, to me, that's a great investment in learning. Uh, it's those those kind of even though that's more than 20 bucks a month for the audio, for for the pipeline, that to me is is where it's a really good spend of money on the TTS, on the LLM, on getting the generations, because it provides really interesting learning content for me that I enjoy listening to. And it's to me that's uh it's very, it's a very worthwhile spend of API money. Um, so it's I want to just get that kind of package going. And then the other pieces of the publishing are kind of the the last stages, I'd say.
